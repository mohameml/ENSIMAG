{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #ffffff; background-color: #2979ff; border-bottom: 1px solid #004ecb; padding: 2em;\">\n",
    "<b style=\"font-size: 20pt;\">TP4: Stochastic gradient & co</b> <br> <br>\n",
    "<b style=\"font-size: 15pt; color: #bbdefb\">Ensimag 2A — Special grading lab</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student(s): Name1 (and Name2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Automatic reload of local libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**[I. Introduction](#I.-Introduction)**<br> \n",
    "**[II. The optimization problem](#II.-The-optimization-problem)**<br>\n",
    "**[III. Implementing the solvers](#III.-Implementing-the-solvers)**<br>\n",
    "&nbsp; &nbsp; &nbsp;[GD (Gradient descent)](#GD-(Gradient-descent))<br>\n",
    "&nbsp; &nbsp; &nbsp;[SGD (Stochastic Gradient Descent)](#SGD-(Stochastic-Gradient-Descent)<br>\n",
    "&nbsp; &nbsp; &nbsp;[SAGA (Stochastic Average Gradient \"Amélioré\")](#SAGA-(Stochastic-Average-Gradient-\"Amélioré\")<br>\n",
    "&nbsp; &nbsp; &nbsp;[Comparison](#Comparison)<br>\n",
    "**[IV. Regularization](#IV.-Regularization)**<br>\n",
    "**[V. Further development](#V.-Further-development)**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will implement optimization algorithms that we've studied together and use them to fit a logistic regression classification model. Parts of this lab intersect TP2 and 3.\n",
    "\n",
    "You will be implementing a simplified version of [`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). \n",
    "\n",
    "Part of the work has already been done for you (see the `\\lib` folder), so you'll only have to implement the `##### TODO (i) #####` sections of the code. Whenever you complete such a section, we ask you to **copy it in this notebook**. We want you to **only send us a pdf or html export of this notebook** (so we won't be able to see your work if it's not in this notebook). \n",
    "\n",
    "Report in groups of 1 to 2 students. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"color: #ffffff; background-color: #2979ff; border-bottom: 5px solid #004ecb; padding: 2em;\"> \n",
    "<h1>I. Introduction</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in previous labs, we consider the [Student Performance](http://archive.ics.uci.edu/ml/datasets/Student+Performance) dataset. This dataset contains student demographic, social, school-related features, how many classes they've missed, etc. \n",
    "\n",
    "The goal we give ourselves is to predict whether a student will pass (final grade > 12).\n",
    "\n",
    "We denote $A \\in \\mathcal{M}_{n,d}(\\mathbb{R})$ the feature matrix, containing $n$ samples with $d$ features, and $b \\in \\{-1,1\\}^n$ the label vector where $-1$ encodes `fail` and $1$ encodes `pass`.\n",
    "\n",
    "The data has already been preprocessed: You can check ```\\lib\\datasets.py``` if you're curious. The following loads the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.datasets import load_student_data\n",
    "\n",
    "A_train, A_test, b_train, b_test = load_student_data('data/student-mat.csv', split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, you're going to implement a simplified version of [`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). So let's first see the type of output it is supposed to give. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe; border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b>  \n",
    "    <ul>\n",
    "        <li> Import <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">sklearn.linear_model.LogisticRegression </a>. </li>\n",
    "        <li> Create an instance of sklearn's logistic regression classifiers with SAGA as its solver. (You may decrease the target accuracy to 0.001) </li>\n",
    "        <li> Fit the classifier on the training data and compute the score of the fitted model on the test set.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"color: #ffffff; background-color: #2979ff; border-bottom: 5px solid #004ecb; padding: 2em;\"> \n",
    "<h1>II. The optimization problem</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression consists in finding the parameter $x \\in \\mathbb{R}^d$ that best fits:\n",
    "$$\n",
    "b_i = \\begin{cases}\n",
    "\\phantom{-}1 \\ \\textrm{ if }\\ \\langle a_{i}, x\\rangle \\ge 0 \\\\\n",
    "-1 \\ \\textrm{ if }\\ \\langle a_{i}, x\\rangle < 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "for all the samples $i$ of the training set (where $a_i$ is the $i^{th}$ line of $A$ and $b_i$ the $i^{th}$ element of $b$). To measure this fit, logistic regression chooses the following loss:\n",
    "\n",
    "$$\n",
    "\\textrm{LogisticLoss}(b_i, \\left\\langle a_{i}, x\\right\\rangle ) = \\log \\left(1+\\exp \\left(-b_{i}\\left\\langle a_{i}, x\\right\\rangle\\right)\\right) \n",
    "$$\n",
    "\n",
    "The corresponding optimization problem we will consider is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #eceff1; border-left: 0px solid #78909c; padding: 2em; border-radius: 5px;\"> \n",
    "\n",
    "Minimization of the empirical risk $f$ of the $L_2$-regularized logistic regression model, on the dataset $(A_{\\textrm{train}},b_{\\textrm{train}})$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textrm{min}_{x\\in \\mathbb{R}^d} \\  f(x) \n",
    "\\ &= \\ \\textrm{min}_{x\\in \\mathbb{R}^d} \\ \\frac{1}{n_{\\textrm{train}}} \\sum_{i=1}^{n_{\\textrm{train}}} f_i(x) \\\\\n",
    "&= \\ \\textrm{min}_{x\\in \\mathbb{R}^d}\\frac{1}{n_{\\textrm{train}}} \\sum_{i=1}^{n_{\\textrm{train}}} \\textrm{LogisticLoss}(b_i, \\left\\langle a_{i}, x\\right\\rangle ) + \\frac{l_2}{2}\\|x\\|^{2}_2,  \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $l_2 \\ge 0$ is the regularization parameter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the notation, we'll stop indexing $A$, $b$ and $n$ with \"$\\textrm{train}$\" in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the functions $f_i$s and $f$ are $C^1$ with $L$-Lischitz gradient, with $L \\le \\frac{\\max_i\\|a_i\\|^2_2}{4} + l_2$ and we can therefore use the following value in our implementation:\n",
    "```python\n",
    "L = 0.25 * max(np.linalg.norm(A,2,axis=1))**2 + self.l2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"color: #ffffff; background-color: #2979ff; border-bottom: 5px solid #004ecb; padding: 2em;\"> \n",
    "<h1>III. Implementing the solvers</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement gradient methods, we need an oracle that will compute both batch gradients $\\nabla f$ and partial gradients $\\nabla f_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe;  border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b>  In the code below, $\\nabla f$ has already been implemented.\n",
    "    <ul>\n",
    "        <li> Complete <b>TODO (1)</b> below.</li>\n",
    "        <li> Add your code to <b>\\lib\\linear_model.py</b>.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def grad(x, i=None):\n",
    "    '''\n",
    "    Gradient of the objective function at x.\n",
    "    If no i is given, grad should return the batch gradient.\n",
    "    Otherwise, it should return the partial gradient associated the the datapoint (A[i], b[i]).\n",
    "    '''\n",
    "\n",
    "    if i is None: # return batch gradient\n",
    "        output = -b/(1 + np.exp(b*np.dot(A, x)))\n",
    "        output = sum(np.diag(output))/n @ A\n",
    "        output += self.l2*x\n",
    "    else: # return partial gradient associated the datapoint (A[i], b[i])\n",
    "        ######## TODO (1) ########\n",
    "        pass\n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GD (Gradient descent) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At iteration k of the GD algorithm, the following update is executed:\n",
    "\n",
    "$$\n",
    "x_{k+1} \\leftarrow x_{k} - \\gamma\\nabla f(x_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe;  border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b>  \n",
    "    <ul>\n",
    "        <li> What stepsize $\\gamma$ ensures the convergence of this algorithm ?</li>\n",
    "        <li> Complete <b>TODO (2)</b> and <b>TODO (3)</b></li>\n",
    "        <li> Add your code to <b>\\lib\\solvers.py</b>.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**\n",
    ">\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def GD(x0, grad, max_iter, n, L, mu):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    x0: array, shape (nb_features,)\n",
    "        Initialisation of the solver\n",
    "    grad: function\n",
    "        Gradient of the objective function\n",
    "    max_iter: int\n",
    "        Number of iterations (i.e. number of descent steps). Note that for GD, \n",
    "        one iteration is one epoch i.e, one pass through the data, while for SGD, SAG and SAGA, \n",
    "        one iteration uses only one data point.\n",
    "    n: int\n",
    "        Dataset size\n",
    "    L: float\n",
    "        Smoothness constant of the objective function\n",
    "    mu: float\n",
    "        Strong convexity constant of the objective function\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x: array, shape (nb_features,)\n",
    "        final iterate of the solver\n",
    "    x_tab: array, shape (nb_features, max_iter)\n",
    "        table of all the iterates\n",
    "    '''\n",
    "    stepsize  = ######## TODO (2) ########\n",
    "    x         = x0\n",
    "    x_tab     = np.copy(x)\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        x     = ######## TODO (3): Implement a single step ########\n",
    "        x_tab = np.vstack((x_tab, x))\n",
    "\n",
    "    return x, x_tab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe;  border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b> Run the code below to test your code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.linear_model import LogisticRegression\n",
    "from lib.visuals import learning_curve\n",
    "\n",
    "clf_gd = LogisticRegression(solver='gd', l2=0.01, max_iter=100)\n",
    "clf_gd.fit(A_train, b_train)\n",
    "\n",
    "learning_curve(GD=clf_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD (Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At iteration k of the SGD algorithm, the following update is executed:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "i \\textrm{ chosen randomly in } \\{1...n\\}\\\\\n",
    "x_{k+1} \\leftarrow x_{k} - \\gamma_k \\nabla f_i(x_k) \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe;  border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b>  \n",
    "    <ul>\n",
    "        <li> What stepsize $\\gamma_k$ ensures the convergence of this algorithm and under what assumptions?</li>\n",
    "        <li> Complete <b>TODO (4)</b></li>\n",
    "        <li> Add your code to <b>\\lib\\solvers.py</b>.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def SGD(x0, grad, max_iter, n, L, mu):\n",
    "    ######## TODO (4) ########\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        ######## TODO (4) ########\n",
    "        \n",
    "        if k%n == 0: # each completed epoch\n",
    "            x_tab = np.vstack((x_tab, x))\n",
    "\n",
    "    return x, x_tab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe;  border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b> Run the code below to test your code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_sgd = LogisticRegression(solver='sgd', l2=0.01, max_iter=100*len(b_train))\n",
    "clf_sgd.fit(A_train, b_train)\n",
    "\n",
    "learning_curve(SGD=clf_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAGA (Stochastic Average Gradient \"Amélioré\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAGA starts by computing a single batch gradient, then at iteration k, the following update is executed:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "i \\textrm{ chosen randomly in } \\{1...n\\}\\\\\n",
    "x_{k+1} \\leftarrow x_{k} - \\gamma_k\\big(\\nabla f_i(x_k) - \\alpha_i + \\bar{\\alpha}\\big) \\\\\n",
    "\\alpha_i \\leftarrow \\nabla f_i(x) \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $(\\alpha_j)_{j=1}^n$ are gradients evaluated at previous iterates and $\\bar{\\alpha}=\\frac{1}{n}\\sum^n_{j=1}\\alpha_j$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe;  border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b>  \n",
    "    <ul>\n",
    "        <li> What stepsize $\\gamma_k$ ensures the convergence of this algorithm and under what assumptions?</li>\n",
    "        <li> Complete <b>TODO (5)</b></li>\n",
    "        <li> Add your code to <b>\\lib\\solvers.py</b>.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def SAGA(x0, grad, max_iter, n, L, mu):\n",
    "    ######## TODO (5) ########\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        ######## TODO (5) ########\n",
    "        \n",
    "        if k%n == 0: # each completed epoch\n",
    "            x_tab = np.vstack((x_tab, x))\n",
    "\n",
    "    return x, x_tab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe;  border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b> Run the code below to test your code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_saga = LogisticRegression(solver='saga', l2=0.01, max_iter=30*len(b_train))\n",
    "clf_saga.fit(A_train, b_train)\n",
    "\n",
    "learning_curve(SAGA=clf_saga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe;  border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b> Run the code below to compare all the methods you've implemented.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "n = len(b_train)\n",
    "\n",
    "clf_gd   = LogisticRegression(solver='gd', l2=0.01, max_iter=epochs)\n",
    "clf_sgd  = LogisticRegression(solver='sgd', l2=0.01, max_iter=epochs*n)\n",
    "clf_saga = LogisticRegression(solver='saga', l2=0.01, max_iter=epochs*n)\n",
    "\n",
    "clf_gd.fit(A_train, b_train)\n",
    "clf_sgd.fit(A_train, b_train)\n",
    "clf_saga.fit(A_train, b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve(GD=clf_gd, SGD=clf_sgd, SAGA=clf_saga)\n",
    "\n",
    "for clf, name in zip([clf_gd, clf_sgd, clf_saga], ['gd', 'sgd', 'saga']):\n",
    "    print(name + ':' \\\n",
    "          + \"\\tAccuracy on training set: {0:0.1f}%\".format(100*clf.score(A_train, b_train)))\n",
    "    print(\"\\tAccuracy on testing set:  {0:0.1f}%\".format(100*clf.score(A_test, b_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"color: #ffffff; background-color: #2979ff; border-bottom: 5px solid #004ecb; padding: 2em;\"> \n",
    "<h1>IV. Regularization</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe;  border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b> \n",
    "    <ul>\n",
    "        <li> Rewrite the empirical risk minimization objective with both $L_1$ and $L_2$ regularization (with coefficients $l_1$ and $l_2$).</li>\n",
    "        <li> Is this objective $\\mu$-strongly convex, $L$-smooth, differentiable?</i>\n",
    "        <li> Complete the <b>TODO (6)</b> to account for the new term in the objective.</i>\n",
    "        <li> Add your code to <b>lib/linear_model.py</b>.</i>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "self._empirical_risk = lambda x: sum(self._logistic_loss(A, b, x))/n \\\n",
    "                                 + self.l2/2.0 * np.linalg.norm(x,2) \\\n",
    "                                 + ######## TODO (6) ########\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimization algorithms we've implemented have a slow rate of convergence for this new objective function. We will therefore use *proximal* variations of these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe;  border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b> Using the $\\textrm{prox}_{l_1 \\|\\cdot\\|_1}$ operator, write down the $k^{th}$ step of proximal GD, proximal SGD, and proximal SAGA.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe;  border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b> \n",
    "    <ul>\n",
    "        <li> Give a closed-form expression of $\\textrm{prox}_{l_1 \\|\\cdot\\|_1}$</li>\n",
    "        <li> Complete <b>TODO (7)</b> (note that you can access the $l_1$ coefficient using <b>self.l1</b>).</li>\n",
    "        <li> Add your code to <b>\\lib\\linear_model.py</b>.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def prox(x, stepsize):\n",
    "    ######## TODO (7) ########\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe;  border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b> \n",
    "    <ul>\n",
    "        <li> Add <b>prox</b> to the attributes of the solvers (both in the call to <b>self.solver</b> in <b>/lib/linear_model.py</b> and in <b>/lib/solvers.py</b>) as follows: <br> solver(x0, grad, <b>prox</b>, self.max_iter, n, L, mu)</li>\n",
    "        <li> Add a proximal step to all the algorithms in <b>/lib/solvers.py</b>. </li>\n",
    "        <li> Paste one of these algorithms in the cell bellow.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def {GD or SGD or SAGA}(x0, grad, prox, max_iter, n, L, mu):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe;  border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b> If your proximal gradient is correctly implemented, the following cell plots the support of the solution for various values of $l_1$.\n",
    "    <ul>\n",
    "        <li> Interpret the plot and explain what can $L_1$ regularization be used for.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.visuals import l1_regularization_plot\n",
    "\n",
    "clf = lambda l1: LogisticRegression(solver='gd', l1=l1, l2=0.1, max_iter=1000)\n",
    "\n",
    "l1_regularization_plot(clf, A_train, b_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1f5fe;  border-left: 5px solid #2962ff; padding: 0.5em;\"> \n",
    "<b style=\"color: #2962ff;\">Question:</b> The following cell will plot the value of the coordinates of the solution for various values of $l_2$.\n",
    "    <ul>\n",
    "        <li> Interpret the plot and explain what can $L_2$ regularization be used for.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.visuals import l2_regularization_plot\n",
    "\n",
    "clf = lambda l2: LogisticRegression(solver='gd', l1=0.01, l2=l2, max_iter=1000)\n",
    "\n",
    "l2_regularization_plot(clf, A_train, b_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"color: #ffffff; background-color: #2979ff; border-bottom: 5px solid #004ecb; padding: 2em;\"> \n",
    "<h1>V. Further development</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect that you go beyond these questions and that you develop certain aspects, depending on your personal interests and skills, for instance:\n",
    "- Implementations and numerical tests (further developments, more experiments, etc.)\n",
    "- Applications in learning or statistics (interpretation of some results, other models, other datasets, etc.)\n",
    "- Theoretical or mathematical questions (convergence proofs, convergence rates, advanced versions of an algorithm, theoretical analysis of special cases, etc.) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
